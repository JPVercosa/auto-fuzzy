{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoFIS code experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.stats as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import fbeta_score, make_scorer\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# data visualization\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('seaborn')\n",
    "\n",
    "# other tools\n",
    "# from utils import dataframe_utils, preprocess_utils, feature_importance_utils, model_utils, preprocess_utils\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing benchmark dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       0    1    2    3  target\n0    5.1  3.5  1.4  0.2       0\n1    4.9  3.0  1.4  0.2       0\n2    4.7  3.2  1.3  0.2       0\n3    4.6  3.1  1.5  0.2       0\n4    5.0  3.6  1.4  0.2       0\n..   ...  ...  ...  ...     ...\n145  6.7  3.0  5.2  2.3       2\n146  6.3  2.5  5.0  1.9       2\n147  6.5  3.0  5.2  2.0       2\n148  6.2  3.4  5.4  2.3       2\n149  5.9  3.0  5.1  1.8       2\n\n[150 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(iris['data'])\n",
    "df_iris['target'] = iris['target']\n",
    "df_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.autoFIS.autoFIS.data_preparation import DataPreparation\n",
    "from main.autoFIS.autoFIS.formulation import Formulation\n",
    "from main.autoFIS.autoFIS.association import Association\n",
    "from main.autoFIS.autoFIS.aggregation import Aggregation\n",
    "from main.autoFIS.autoFIS.decision import Decision\n",
    "from main.autoFIS.autoFIS.evaluation import Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01 - Prepare Data (this step will be eliminated in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from main.autoFIS.autoFIS.data_preparation import DataPreparation\n",
    "# reader = DataPreparation()\n",
    "# _, target_class_one_hot, frequency_of_classes, count_of_classes,_ = reader.calculate_parameters(df_iris)\n",
    "\n",
    "# X, cBin, fClasses, dictFreq, _ = reader.calculate_parameters(df_iris)\n",
    "# Pelo oq eu entendi aqui, tem q fazer o one hot da classe de saída também."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are set by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attributes = [False, False, False, False]\n",
    "triangle_format = 'normal'  # \"tukey\", \"normal\"\n",
    "fuzzy_sets_by_attribute = 5  # 3, 5, 7\n",
    "enable_negation = False\n",
    "\n",
    "# -------------------------\n",
    "# Formulation parameters\n",
    "# -------------------------\n",
    "# ordem_max_premises = 2\n",
    "premise_max_size = 2\n",
    "t_norm = 'prod'  # \"min\", \"prod\"\n",
    "\n",
    "# Area filter parameters:\n",
    "criteria_support = \"cardinalidade_relativa\"  # \"cardinalidade_relativa\", \"frequencia_relativa\"\n",
    "area_threshold = 0.05\n",
    "\n",
    "# PCD filter parameter:\n",
    "# is_enable_pcd = [True, False]\n",
    "\n",
    "enable_pcd_premises_base = True\n",
    "enable_pcd_premises_derived = True\n",
    "\n",
    "# Overlapping filter parameters:\n",
    "enable_similarity_premises_bases = True\n",
    "enable_similarity_premises_derived = True\n",
    "\n",
    "is_enable_overlapping = [True, True]\n",
    "threshold_similarity = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par_area = [criteria_area, area_threshold]\n",
    "# par_over = [is_enable_overlapping, threshold_similarity]\n",
    "# par_pcd = is_enable_pcd\n",
    "target_class_one_hot = pd.get_dummies(y_train).values\n",
    "number_classes = target_class_one_hot.shape[1]\n",
    "percentage_of_classes = target_class_one_hot.sum(axis = 0) / target_class_one_hot.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Avaliar melhor forma de iniciar classes e passar parâmetros para métodos internos.\n",
    "\n",
    "fuzzification_params = {\n",
    "    'X' :  X_train,\n",
    "    'categorical_attributes_mask' : categorical_attributes,\n",
    "    'triangle_format' : triangle_format,\n",
    "    'fuzzy_sets_by_attribute': fuzzy_sets_by_attribute,\n",
    "    'enable_negation': enable_negation\n",
    "}\n",
    "\n",
    "# TODO: Consertar fuzzificação. o output precisa ser um mapping de entradas crispy para fuzzysets\n",
    "fuzzifier = Fuzzification(**fuzzification_params)\n",
    "fuzzifier.build_membership_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done with Formulation...\nDepth level 1: 18\nDepth level 2: 39\n"
    }
   ],
   "source": [
    "# ref_attributes é um vetor contando os atributos da base. pode ser removido\n",
    "\n",
    "formulation_params = {\n",
    "    'ux': fuzzifier.uX, \n",
    "    'tnorm':t_norm,\n",
    "    'target_class': target_class_one_hot, \n",
    "    'num_of_attributes': X_train.shape[1], \n",
    "    'antecedents_by_attribute': fuzzifier.antecedents_by_attribute, \n",
    "    'num_of_antecedents_by_attribute': fuzzifier.num_of_antecedents_by_attribute, \n",
    "    'attributes_negation_mask': fuzzifier.attributes_negation_mask,\n",
    "    'premise_max_size' : premise_max_size,\n",
    "    'criteria_support' : criteria_support,  # 'cardinalidade relativa', 'frequencia relativa'\n",
    "    'threshold_support' : area_threshold,  # tolerancia da area\n",
    "    'enable_similarity_premises_bases' : enable_similarity_premises_bases,\n",
    "    'enable_similarity_premises_derived' : enable_similarity_premises_derived,\n",
    "    'threshold_similarity' : threshold_similarity,\n",
    "    'enable_pcd_premises_base' : enable_pcd_premises_base,\n",
    "    'enable_pcd_premises_derived' : enable_pcd_premises_derived\n",
    "}\n",
    "\n",
    "formulator = Formulation(**formulation_params)\n",
    "tree = formulator.generate_premises()\n",
    "\n",
    "#TODO: Colocar essas mensagens dentro de alguma etapa de validação de premissas.\n",
    "status = [False if not i[0] else True for i in tree] # checa se tem alguma ordem de premisas vazia\n",
    "sum_status = sum(status)\n",
    "if sum_status != len(tree): \n",
    "    if sum_status == 0:\n",
    "        raise ValueError(\"Error in Formulation Module. Any premise survived. \"\n",
    "                            \"Sorry, you can not continue in the next stage.\"\n",
    "                            \"\\nTry to change the configuration\")\n",
    "    else:\n",
    "        # TODO: Consertar isso. Acho que ele filtra só as ordens de premissas que estão preenchidas\n",
    "        arb = [i for i in tree if i[0]]\n",
    "        tree, arb = arb, tree\n",
    "\n",
    "print('Done with Formulation...')\n",
    "for i,values in enumerate(tree):\n",
    "    print('Depth level ' + str(i + 1) + ': ' + str(tree[i][1].shape[1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_method = \"MQR\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq_max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done with Association...\n\nRules per class:\nclass  0 :  [(1,), (10,), (15,), (10, 15)]\nclass  1 :  [(12,), (17,), (12, 17)]\nclass  2 :  [(13,), (18,), (19,), (13, 18), (13, 19)]\n"
    }
   ],
   "source": [
    "#TODO: Se sobrar tempo, fazer class polymorphismo nos métodos de agregação\n",
    "#TODO: Se sobrar tempo [2], remover import de auxfunc e deixar tudo dentro de uma classe só. A menos q esteja sendo usado em outro lugar a mesma função.\n",
    "\n",
    "associator = Association(tree, target_class_one_hot)\n",
    "# premises_ux_by_class = associator.division(association_method)\n",
    "association_rules = associator.build_association_rules(association_method)\n",
    "\n",
    "#TODO: Colocar essas mensagens dentro de alguma etapa de validação de premissas.\n",
    "status = [0 if not i[0] else 1 for i in association_rules]\n",
    "if sum(status) != number_classes:\n",
    "    raise ValueError(\"Error in Division Module. Some classes did not get premises. \"\n",
    "                        \"Sorry, you can not continue in the next stage.\"\n",
    "                        \"\\nTry to change the configuration\")\n",
    "print('Done with Association...')\n",
    "print('')\n",
    "print('Rules per class:')\n",
    "\n",
    "for index,i in enumerate(association_rules):\n",
    "    print('class ', index, ': ', i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_method = \"MQR\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq_max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done with Aggregation...\n\nPremises of Class 0: [(1,), (10,), (15,)]\nweights_0: [[0.02410548 0.62684905 0.34904547]]\n\nPremises of Class 1: [(12,), (17,)]\nweights_1: [[0.40570462 0.5942954 ]]\n\nPremises of Class 2: [(13,), (18,), (19,)]\nweights_2: [[0.44368515 0.44333386 0.11298105]]\n\n"
    }
   ],
   "source": [
    "#TODO: Se sobrar tempo, fazer class polymorphismo nos métodos de agregação\n",
    "#TODO: Se sobrar tempo [2], remover import de auxfunc e deixar tudo dentro de uma classe só. A menos q esteja sendo usado em outro lugar a mesma função.\n",
    "aggregator = Aggregation(association_rules, target_class_one_hot)\n",
    "premises_weights_names,estimation_classes  = aggregator.aggregate_rules(aggregation_method)\n",
    "\n",
    "#TODO: Colocar essas mensagens dentro de alguma etapa de validação de premissas.\n",
    "status = [0 if not i[0] else 1 for i in premises_weights_names]\n",
    "if sum(status) != number_classes:\n",
    "    raise ValueError(\"Error in Aggregation Module. Some classes did not get premises. \"\n",
    "                        \"Sorry, you can not continue in the next stage.\"\n",
    "                        \"\\nTry to change the configuration\")\n",
    "print('Done with Aggregation...')\n",
    "print('')\n",
    "\n",
    "final_premises_classes = []\n",
    "for index,i in enumerate(premises_weights_names):\n",
    "    print(\"Premises of Class \" + str(index) + \": \" + str(i[0]))\n",
    "    final_premises_classes.append(i[0])\n",
    "    print(\"weights_\" + str(index) + \": \" + str(i[1].T))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done with Decision...\n"
    }
   ],
   "source": [
    "decision_maker = Decision(estimation_classes, percentage_of_classes)\n",
    "train_prediction = decision_maker.dec_max_pert()\n",
    "\n",
    "print('Done with Decision...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9619047619047619"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "accuracy_score(train_prediction, target_class_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[[(1,), (10,), (15,)],\n  array([[0.02410548],\n         [0.62684905],\n         [0.34904547]]),\n  'MQR'],\n [[(12,), (17,)],\n  array([[0.40570462],\n         [0.5942954 ]]),\n  'MQR'],\n [[(13,), (18,), (19,)],\n  array([[0.44368515],\n         [0.44333386],\n         [0.11298105]]),\n  'MQR']]"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "premises_weights_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cbin_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-b7e931fb7d7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 7. Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpremises_weights_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_premises_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentage_of_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmetrics_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbin_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_bin_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmetrics_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbin_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mux_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done with Evaluation...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cbin_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Evaluation\n",
    "f6 = Evaluation(premises_weights_names, final_premises_classes, percentage_of_classes)\n",
    "metrics_train = f6.eval_train(cbin_train, train_bin_prediction)\n",
    "metrics_test = f6.eval_test(cbin_test, ux_test, t_norm)\n",
    "print('Done with Evaluation...')\n",
    "\n",
    "report.append(\"\\nEvaluation Training:\\n---------------------------\")\n",
    "report.append(\"Accuracy on train dataset: \" + str(metrics_train[0]))\n",
    "report.append(\"AUC in train dataset: \" + str(metrics_train[1]))\n",
    "report.append(\"Recall: \" + str(metrics_train[3]))\n",
    "report.append('Confusion matrix:\\n' + str(metrics_train[2]))\n",
    "\n",
    "report.append(\"\\nEvaluation Testing:\\n---------------------------\")\n",
    "report.append(\"Accuracy on test dataset: \" + str(metrics_test[0]))\n",
    "report.append(\"AUC in test dataset: \" + str(metrics_test[1]))\n",
    "report.append(\"Recall: \" + str(metrics_test[3]))\n",
    "report.append(\"Confusion matrix:\\n\" + str(metrics_test[2]))\n",
    "\n",
    "# Metrics to eval: accuracy_test, auc_test,\n",
    "#                  [num_regras, total_rule_length, tamano_medio_das_regras]]\n",
    "metricas = [1, [metrics_train[0], metrics_test[0], metrics_train[1], metrics_test[1], metrics_test[4]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'senfis'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-4349c180e637>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msenfis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoFIS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoFIS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoFIS_one_zip\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv_onezip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'senfis'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from senfis.autoFIS.autoFIS.autoFIS_one_zip import cv_onezip\n",
    "import timeit\n",
    "\n",
    "\n",
    "# # # # # # # \n",
    "# Run autoFIS old method\n",
    "# # # # # # # \n",
    "\n",
    "def define_parameters(pars):\n",
    "    try:\n",
    "        parameters = [categorical_mask] + pars  # addition of 2 list\n",
    "    except KeyError:\n",
    "        print (\"The database \" + database_name + \" was not found.\\nIt was assumed that all attributes are numeric\")\n",
    "        parameters = [0]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def run_autofis(current_folder_path):\n",
    "    # -------------------------\n",
    "    # Fuzzification parameters\n",
    "    # -------------------------\n",
    "    # categorical_bool_attributes = [0, 0, 0, 0, 1, 0, 0, 0, 0]  # <<=====\n",
    "    triangular_fuzzification_type = \"tukey\"  # \"tukey\", \"normal\"\n",
    "    num_partitions_by_attribute = 3  # 3, 5, 7\n",
    "    is_enable_negation = 0  # 0, 1\n",
    "\n",
    "    # -------------------------\n",
    "    # Formulation parameters\n",
    "    # -------------------------\n",
    "    t_norm = \"prod\"  # \"min\", \"prod\"\n",
    "    ordem_max_premises = 2\n",
    "    # Area filter parameters:\n",
    "    criteria_area = \"cardinalidade_relativa\"  # \"cardinalidade_relativa\", \"frequencia_relativa\"\n",
    "    area_threshold = 0.1\n",
    "    # PCD filter parameter:\n",
    "    is_enable_pcd = [1, 0]\n",
    "    # Overlapping filter parameters:\n",
    "    is_enable_overlapping = [1, 1]\n",
    "    overlapping_threshold = 0.95\n",
    "\n",
    "    # -------------------------\n",
    "    # Association: ex Splitting\n",
    "    # -------------------------\n",
    "    method_association = \"CD\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Aggregation\n",
    "    # -------------------------\n",
    "    method_aggregation = \"MQR\"  # \"MQR\", \"PMQR\", \"intMQR\", \"CD\", \"PCD\", \"max\"\n",
    "\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    # %% Grouping parameters: %%\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    parameters = [triangular_fuzzification_type, num_partitions_by_attribute, t_norm, is_enable_negation,\n",
    "                  ordem_max_premises, criteria_area, area_threshold, is_enable_pcd,\n",
    "                  is_enable_overlapping, overlapping_threshold,\n",
    "                  method_association, method_aggregation]\n",
    "\n",
    "    # =============================================================================================== #\n",
    "    # current_folder_path = os.path.dirname(os.path.realpath(__file__))\n",
    "    current_folder_path = current_folder_path\n",
    "    print (current_folder_path)\n",
    "\n",
    "    databases = []\n",
    "\n",
    "    for archivo in os.listdir(current_folder_path):\n",
    "        if archivo.endswith(\"_csv.zip\"):\n",
    "            databases.append(archivo)\n",
    "    print (databases)\n",
    "    # =============================================================================================== #\n",
    "\n",
    "    file_times = open(os.path.join(current_folder_path, \"Experimento_times.csv\"), 'w')\n",
    "    file_times.write(\"Dataset\" + \", \" + \"Time(s)\" + '\\n')\n",
    "    file_times.close()\n",
    "    # Evaluate each database (zip file)\n",
    "    for data in databases:\n",
    "        t0 = timeit.default_timer()\n",
    "        try:\n",
    "            # data_name_key = data[0:-16]\n",
    "            parameters_database = define_parameters(parameters)\n",
    "            achievement = cv_onezip(current_folder_path, data, parameters_database)\n",
    "            if achievement == 0:\n",
    "                raise ValueError(\"Problems in database: \" + \"<\" + data + \">\")\n",
    "        except ValueError as e:\n",
    "            print (e)\n",
    "            achievement = 0\n",
    "        tf = timeit.default_timer()\n",
    "\n",
    "        if achievement:\n",
    "            file_times = open(os.path.join(current_folder_path, \"Experimento_times.csv\"), 'a')\n",
    "            file_times.write(data[:-16] + ', ' + str(tf - t0) + '\\n')\n",
    "            file_times.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este arquivo executa o processamento de uma base de dados, utilizando validação cruzada.\n",
    "# A base de dados (um arquivo zip) já é separada em 10 splits (em csv) para a validação cruzada.\n",
    "\n",
    "__author__ = 'jparedes'\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from .autoFIS_one_cv import autofis_onecv\n",
    "from numpy import mean, std\n",
    "\n",
    "\n",
    "def cv_onezip(path_databases, zip_file_name, parameters, path_output=0):\n",
    "    # Este arquivo executa o processamento de uma base de dados, utilizando validação cruzada.\n",
    "    # A base de dados (um arquivo zip) já é separada em 10 splits (em csv) para a validação cruzada.\n",
    "    if path_output == 0:\n",
    "        path_output = path_databases\n",
    "    zipFilePath = os.path.join(path_databases, zip_file_name)\n",
    "\n",
    "    # ==================================================================== #\n",
    "    try:\n",
    "        with zipfile.ZipFile(zipFilePath, 'r') as z:\n",
    "            files_cv = z.namelist()\n",
    "\n",
    "        number_files_zip = len(files_cv)\n",
    "        if not (number_files_zip == 20 or number_files_zip == 10):\n",
    "            raise ValueError(\"This module works with a zip file to 10cv or 5cv. \"\n",
    "                             \"For this reason, it is expected 20 or 10 files inside the zip file\")\n",
    "        elif number_files_zip == 20:\n",
    "            a = files_cv[2:] + files_cv[0:2]\n",
    "        else:  # number_files_zip == 10\n",
    "            a = files_cv\n",
    "\n",
    "        list_train, list_test = a[::2], a[1::2]\n",
    "\n",
    "        msg = []\n",
    "        number_cv_pairs = int(number_files_zip / 2)\n",
    "        ac_train = number_cv_pairs * [0]\n",
    "        ac_test = number_cv_pairs * [0]\n",
    "        auc_train = number_cv_pairs * [0]\n",
    "        auc_test = number_cv_pairs * [0]\n",
    "\n",
    "        num_rules = number_cv_pairs * [0]\n",
    "        total_rule_length = number_cv_pairs * [0]\n",
    "\n",
    "        for i in range(number_cv_pairs):\n",
    "            print('Fold nº: ',i)\n",
    "            train_file = list_train[i]\n",
    "            test_file = list_test[i]\n",
    "\n",
    "            message, indicators = autofis_onecv(zipFilePath, train_file, test_file, parameters)\n",
    "            msg.append(message)\n",
    "\n",
    "            if indicators[0] == 0:\n",
    "                name_error = os.path.join(path_output, 'ERROR') + zip_file_name[:-13]\n",
    "                fail_error = open(name_error, 'w')\n",
    "                fail_error.write('Error in CV:' + str(i + 1))\n",
    "                fail_error.write(\"\\n\" + message)\n",
    "                fail_error.close()\n",
    "                raise ValueError(\"Problem detected in CV \" + str(i + 1))\n",
    "\n",
    "            ac_train[i], ac_test[i] = indicators[1][0], indicators[1][1]\n",
    "            auc_train[i], auc_test[i] = indicators[1][2], indicators[1][3]\n",
    "            num_rules[i] = indicators[1][4][0]\n",
    "            total_rule_length[i] = indicators[1][4][1]\n",
    "\n",
    "        filename = os.path.join(path_output, 'Report of ') + zip_file_name[:-8]\n",
    "        target = open(filename, 'w')\n",
    "        target.write('Parameters: ' + str(parameters))\n",
    "        for i2 in range(number_cv_pairs):\n",
    "            target.write('\\n\\n' + str(4 * '===============================') + '\\n\\n')\n",
    "            target.write('CV-' + str(i2 + 1) + '\\n')\n",
    "            target.write('\\n'.join(msg[i2]))\n",
    "\n",
    "        target.write('\\n\\n' + str(4 * '===============================') + '\\n\\n')\n",
    "        target.write('Accuracy training: ' + str(mean(ac_train)) + ', ' + str(std(ac_train)) + '\\n')\n",
    "        target.write('Accuracy testing: ' + str(mean(ac_test)) + ', ' + str(std(ac_test)) + '\\n')\n",
    "        target.write('AUC training: ' + str(mean(auc_train)) + ', ' + str(std(auc_train)) + '\\n')\n",
    "        target.write('AUC testing: ' + str(mean(auc_test)) + ', ' + str(std(auc_test)) + '\\n')\n",
    "        target.write('Number of rules: ' + str(mean(num_rules)) + '\\n')\n",
    "        target.write('Total Rule Length: ' + str(mean(total_rule_length)))\n",
    "        target.close()\n",
    "\n",
    "        achievement = 1\n",
    "\n",
    "        print (\"win \", zip_file_name)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print (e)\n",
    "        achievement = 0\n",
    "\n",
    "    return achievement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este arquivo executa o autoFIS para um fold da validação cruzada\n",
    "\n",
    "import autoFIS.autoFIS.utils_autofis as toolfis\n",
    "# import .utils_autofis as toolfis\n",
    "from autoFIS.autoFIS.formul.autoFIS.formulation import Formulation\n",
    "from autoFIS.autoFIS.association import Association\n",
    "from autoFIS.autoFIS.aggregation import Aggregation\n",
    "from autoFIS.autoFIS.decisions import Decisions\n",
    "from autoFIS.autoFIS.evaluation import Evaluation\n",
    "\n",
    "\n",
    "def autofis_onecv(file_zip, file_train, file_test, parameters):\n",
    "\n",
    "    # General parameters\n",
    "    t_norm = parameters[3]\n",
    "    max_size_of_premise = parameters[5]\n",
    "    association_method = parameters[11]\n",
    "    aggregation_method = parameters[12]\n",
    "\n",
    "    # Gathering parameters\n",
    "    # Formulation parameters:\n",
    "    par_area, par_over, par_pcd = toolfis.get_formulation_parameters(parameters)\n",
    "\n",
    "    # 1. Lecture & Fuzzification\n",
    "    out1 = toolfis.lecture_fuz_one_cv(file_zip, file_train, file_test, parameters)\n",
    "    ux_train, cbin_train = out1[0]\n",
    "    ux_test, cbin_test = out1[1]\n",
    "    num_premises_by_attribute, premises_by_attribute, ref_attributes, premises_contain_negation = out1[2]\n",
    "    freq_classes = out1[3]\n",
    "\n",
    "    report = []  # To save our results\n",
    "\n",
    "    try:\n",
    "        # 3. Formulation\n",
    "        f2 = Formulation(ux_train, cbin_train, ref_attributes, premises_by_attribute,\n",
    "                         num_premises_by_attribute, premises_contain_negation)\n",
    "        # Inputs given by user\n",
    "        arbol = f2.gen_ARB(max_size_of_premise, t_norm, par_area, par_over, par_pcd)\n",
    "\n",
    "        status = [0 if not i[0] else 1 for i in arbol]\n",
    "        sum_status = sum(status)\n",
    "        if sum_status != len(arbol):\n",
    "            if sum_status == 0:\n",
    "                raise ValueError(\"Error in Formulation Module. Any premise survived. \"\n",
    "                                 \"Sorry, you can not continue in the next stage.\"\n",
    "                                 \"\\nTry to change the configuration\")\n",
    "            else:\n",
    "                arb = [i for i in arbol if i[0]]\n",
    "                arbol, arb = arb, arbol\n",
    "        print('Done with Formulation...')\n",
    "        number_classes = cbin_train.shape[1]\n",
    "\n",
    "        report.append(\"\\nFormulation:\\n-----------------\")\n",
    "        report.append(\"Elementos acorde a la profundidad \" + str(len(arbol)) + \" del arbol\")\n",
    "        for i in range(len(arbol)):\n",
    "            report.append('Profundidad ' + str(i + 1) + ': ' + str(arbol[i][1].shape))\n",
    "            # print 'Profundidad ' + str(i + 1) + ': ' + str(arbol[i][1].shape)\n",
    "\n",
    "        # 4. Association: ex-Division\n",
    "        f3 = Association(arbol, cbin_train)\n",
    "        premises_ux_by_class = f3.division(association_method)\n",
    "\n",
    "        status = [0 if not i[0] else 1 for i in premises_ux_by_class]\n",
    "        if sum(status) != number_classes:\n",
    "            raise ValueError(\"Error in Division Module. Some classes did not get premises. \"\n",
    "                             \"Sorry, you can not continue in the next stage.\"\n",
    "                             \"\\nTry to change the configuration\")\n",
    "        print('Done with Association...')\n",
    "\n",
    "        # 5. Aggregation:\n",
    "        f4 = Aggregation(premises_ux_by_class, cbin_train)\n",
    "        output_aggregation = f4.aggregation(aggregation_method)\n",
    "\n",
    "        premises_weights_names = output_aggregation[0]\n",
    "        estimation_classes = output_aggregation[1]\n",
    "\n",
    "        status = [0 if not i[0] else 1 for i in premises_weights_names]\n",
    "        if sum(status) != number_classes:\n",
    "            raise ValueError(\"Error in Aggregation Module. Some classes did not get premises. \"\n",
    "                             \"Sorry, you can not continue in the next stage.\"\n",
    "                             \"\\nTry to change the configuration\")\n",
    "        print('Done with Aggregation...')\n",
    "\n",
    "        final_premises_classes = []\n",
    "        report.append(\"\\n\\nPremises:\\n=========\")\n",
    "        for i in range(len(premises_weights_names)):\n",
    "            report.append(\"Premises of Class \" + str(i) + \": \" + str(premises_weights_names[i][0]))\n",
    "            final_premises_classes.append(premises_weights_names[i][0])\n",
    "            report.append(\"weights_\" + str(i) + \": \" + str(premises_weights_names[i][1].T))\n",
    "\n",
    "        # 6. Decision:\n",
    "        f5 = Decisions(estimation_classes, freq_classes)\n",
    "        train_bin_prediction = f5.dec_max_pert()\n",
    "        print('Done with Decision...')\n",
    "\n",
    "        # 7. Evaluation\n",
    "        f6 = Evaluation(premises_weights_names, final_premises_classes, freq_classes)\n",
    "        metrics_train = f6.eval_train(cbin_train, train_bin_prediction)\n",
    "        metrics_test = f6.eval_test(cbin_test, ux_test, t_norm)\n",
    "        print('Done with Evaluation...')\n",
    "\n",
    "        report.append(\"\\nEvaluation Training:\\n---------------------------\")\n",
    "        report.append(\"Accuracy on train dataset: \" + str(metrics_train[0]))\n",
    "        report.append(\"AUC in train dataset: \" + str(metrics_train[1]))\n",
    "        report.append(\"Recall: \" + str(metrics_train[3]))\n",
    "        report.append('Confusion matrix:\\n' + str(metrics_train[2]))\n",
    "\n",
    "        report.append(\"\\nEvaluation Testing:\\n---------------------------\")\n",
    "        report.append(\"Accuracy on test dataset: \" + str(metrics_test[0]))\n",
    "        report.append(\"AUC in test dataset: \" + str(metrics_test[1]))\n",
    "        report.append(\"Recall: \" + str(metrics_test[3]))\n",
    "        report.append(\"Confusion matrix:\\n\" + str(metrics_test[2]))\n",
    "\n",
    "        # Metrics to eval: accuracy_test, auc_test,\n",
    "        #                  [num_regras, total_rule_length, tamano_medio_das_regras]]\n",
    "        metricas = [1, [metrics_train[0], metrics_test[0], metrics_train[1], metrics_test[1], metrics_test[4]]]\n",
    "\n",
    "    except ValueError as e:\n",
    "        print (e)\n",
    "        report = e  # .append(\"\\n\" + str(e))\n",
    "        metricas = [0, \"No se termino el proceso, se detuvo en algun etapa\"]\n",
    "\n",
    "    return report, metricas\n",
    "\n",
    "\n",
    "def main():\n",
    "    filezip_name = \"D:\\\\Jorg\\Projects\\\\autoFIS\\\\test\\\\datas\" + '\\\\' + 'saheart-10-fold_csv.zip'\n",
    "    train_file = \"saheart-10-7tra.csv\"\n",
    "    test_file = \"saheart-10-7tst.csv\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Fuzzification parameters\n",
    "    # -------------------------\n",
    "    categorical_bool_attributes = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    triangular_fuzzification_type = \"normal\"  # \"tukey\", \"normal\"\n",
    "    num_partitions_by_attribute = 3\n",
    "    t_norm = \"min\"  # \"min\", \"prod\"\n",
    "    is_enable_negation = 0  # 0, 1\n",
    "\n",
    "    # -------------------------\n",
    "    # Formulation parameters\n",
    "    # -------------------------\n",
    "    ordem_max_premises = 2\n",
    "    # Area filter parameters:\n",
    "    criteria_area = \"cardinalidade_relativa\"  # \"cardinalidade_relativa\", \"frequencia_relativa\"\n",
    "    area_threshold = 0.05\n",
    "    # PCD filter parameter:\n",
    "    is_enable_pcd = 1\n",
    "    # Overlapping filter parameters:\n",
    "    is_enable_overlapping = 1\n",
    "    overlapping_threshold = 0.95\n",
    "\n",
    "    # -------------------------\n",
    "    # Association\n",
    "    # -------------------------\n",
    "    method_association = \"MQR\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq_max\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Aggregation\n",
    "    # -------------------------\n",
    "    method_aggregation = \"MQR\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq_max\"\n",
    "\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    # %% Grouping parameters: %%\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    parameters = [categorical_bool_attributes, triangular_fuzzification_type,\n",
    "                  num_partitions_by_attribute, t_norm, is_enable_negation,\n",
    "                  ordem_max_premises, criteria_area, area_threshold, is_enable_pcd,\n",
    "                  is_enable_overlapping, overlapping_threshold,\n",
    "                  method_association, method_aggregation]\n",
    "\n",
    "    result_1cv = autofis_onecv(filezip_name, train_file, test_file, parameters)\n",
    "    print (result_1cv[1])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodos auxiliares\n",
    "\n",
    "def get_formulation_parameters(parameters):\n",
    "    criteria_area = parameters[6]\n",
    "    area_threshold = parameters[7]\n",
    "    is_enable_pcd = parameters[8]\n",
    "    is_enable_overlapping = parameters[9]\n",
    "    overlapping_threshold = parameters[10]\n",
    "    par_area = [criteria_area, area_threshold]\n",
    "    par_over = [is_enable_overlapping, overlapping_threshold]\n",
    "    par_pcd = is_enable_pcd\n",
    "    return par_area, par_over, par_pcd\n",
    "\n",
    "\n",
    "def lecture_fuz_one_cv(zipFilePath, file_train, file_test, parameters):\n",
    "    # Parameters\n",
    "    cat_bool, fz_type, fz_number_partition = parameters[0:3]\n",
    "    is_enable_negation = parameters[4]\n",
    "    # Lecture\n",
    "    reader = Lecture()\n",
    "    reader.read_1cv(zipFilePath, file_train, file_test)\n",
    "    # [x, y_Bin, Freq_Class, Dic_Labels, Dic_Class, Index_Train]\n",
    "    x, y_bin, freq_classes, _, _, index_train = reader.info_data()\n",
    "\n",
    "    # Fuzzification\n",
    "    matrix_x = x.copy()\n",
    "    fuzzifier = Fuzzification(matrix_x, cat_bool)\n",
    "    fuzzifier.build_uX(fz_type, fz_number_partition)\n",
    "    if is_enable_negation == 1:\n",
    "        fuzzifier.add_negation()\n",
    "    print('Successful FZ')\n",
    "    # Getting train and test partitions\n",
    "    ux_train = fuzzifier.uX[:index_train, :]\n",
    "    ux_test = fuzzifier.uX[index_train:, :]\n",
    "    cbin_train = y_bin[:index_train, :]\n",
    "    cbin_test = y_bin[index_train:, :]\n",
    "\n",
    "    # Information about attributes fuzzification\n",
    "    sizes_attributes = fuzzifier.num_of_premises_by_attribute  # [3, 2, 3]\n",
    "    # [(0,1,2),(3,4),(5,6,7)]\n",
    "    premises_by_attribute = fuzzifier.attribute_premises\n",
    "    ref_attributes = fuzzifier.ref_attributes  # [0, 1, 2]\n",
    "    premises_contain_negation = fuzzifier.indexes_premises_contain_negation\n",
    "\n",
    "    fuz_train = [ux_train, cbin_train]\n",
    "    fuz_test = [ux_test, cbin_test]\n",
    "    attributes_information = [\n",
    "        sizes_attributes, premises_by_attribute, ref_attributes, premises_contain_negation]\n",
    "# return fuz_train, fuz_test, attributes_information, freq_classes, gain_by_att\n",
    "    return fuz_train, fuz_test, attributes_information, freq_classes"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('puc': venv)",
   "language": "python",
   "name": "python37764bitpucvenv3c8e04841bf343089962c3369eba30a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}