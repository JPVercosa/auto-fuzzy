{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoFIS code experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# other tools\n",
    "from utils import dataframe_utils, preprocess_utils, feature_importance_utils, model_utils, preprocess_utils\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing benchmark dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       0    1    2    3  target\n0    5.1  3.5  1.4  0.2       0\n1    4.9  3.0  1.4  0.2       0\n2    4.7  3.2  1.3  0.2       0\n3    4.6  3.1  1.5  0.2       0\n4    5.0  3.6  1.4  0.2       0\n..   ...  ...  ...  ...     ...\n145  6.7  3.0  5.2  2.3       2\n146  6.3  2.5  5.0  1.9       2\n147  6.5  3.0  5.2  2.0       2\n148  6.2  3.4  5.4  2.3       2\n149  5.9  3.0  5.1  1.8       2\n\n[150 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(iris['data'])\n",
    "df_iris['target'] = iris['target']\n",
    "df_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoFIS.autoFIS.lecture import Lecture\n",
    "from autoFIS.autoFIS.fuzzification import Fuzzification\n",
    "from autoFIS.autoFIS.formulation.formulation import Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01 - Prepare Data (this step will be eliminated in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Lecture()\n",
    "_, target_class_one_hot, frequency_of_classes, count_of_classes, _ = reader.calculate_parameters(df_iris)\n",
    "# X, cBin, fClasses, dictFreq, _ = reader.calculate_parameters(df_iris)\n",
    "# Pelo oq eu entendi aqui, tem q fazer o one hot da classe de saída também.\n",
    "\n",
    "# Talvez, vale a pena criar um Encoder com One Hot para os atributos categóricos e para a saída\n",
    "# E encapsular isso. Algo semelhante a esse módulo de leitura de dados, mas melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzification (parameters are set by the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attributes = [0, 0, 0, 0]  # <<=====\n",
    "fuzzification_type = 'normal'  # \"tukey\", \"normal\"\n",
    "fuzzy_sets_by_attribute = 3  # 3, 5, 7\n",
    "enable_negation = False\n",
    "\n",
    "# -------------------------\n",
    "# Formulation parameters\n",
    "# -------------------------\n",
    "# ordem_max_premises = 2\n",
    "max_size_of_premise = 2\n",
    "t_norm = 'prod'  # \"min\", \"prod\"\n",
    "# Area filter parameters:\n",
    "criteria_area = \"cardinalidade_relativa\"  # \"cardinalidade_relativa\", \"frequencia_relativa\"\n",
    "area_threshold = 0.05\n",
    "# PCD filter parameter:\n",
    "is_enable_pcd = [1, 0]\n",
    "# Overlapping filter parameters:\n",
    "is_enable_overlapping = [1, 1]\n",
    "overlapping_threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzifier = Fuzzification()\n",
    "fuzzifier.build_membership_functions(X, categorical_attributes,fuzzification_type, fuzzy_sets_by_attribute, enable_negation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_area = [criteria_area, area_threshold]\n",
    "par_over = [is_enable_overlapping, overlapping_threshold]\n",
    "par_pcd = is_enable_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ux_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e304653c2407>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m f2 = Formulation(ux_train, cbin_train, ref_attributes, premises_by_attribute,\n\u001b[0m\u001b[0;32m      2\u001b[0m                          num_premises_by_attribute, premises_contain_negation)\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Inputs given by user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0marbol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_premises\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_size_of_premise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpar_area\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpar_over\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpar_pcd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ux_train' is not defined"
     ]
    }
   ],
   "source": [
    "f2 = Formulation(ux_train, cbin_train, ref_attributes, premises_by_attribute,\n",
    "                         num_premises_by_attribute, premises_contain_negation)\n",
    "\n",
    "# Inputs given by user\n",
    "arbol = f2.generate_premises(max_size_of_premise, t_norm, par_area, par_over, par_pcd)\n",
    "\n",
    "status = [0 if not i[0] else 1 for i in arbol]\n",
    "sum_status = sum(status)\n",
    "if sum_status != len(arbol):\n",
    "    if sum_status == 0:\n",
    "        raise ValueError(\"Error in Formulation Module. Any premise survived. \"\n",
    "                            \"Sorry, you can not continue in the next stage.\"\n",
    "                            \"\\nTry to change the configuration\")\n",
    "    else:\n",
    "        arb = [i for i in arbol if i[0]]\n",
    "        arbol, arb = arb, arbol\n",
    "print('Done with Formulation...')\n",
    "number_classes = cbin_train.shape[1]\n",
    "\n",
    "report.append(\"\\nFormulation:\\n-----------------\")\n",
    "report.append(\"Elementos acorde a la profundidad \" + str(len(arbol)) + \" del arbol\")\n",
    "for i in range(len(arbol)):\n",
    "    report.append('Profundidad ' + str(i + 1) + ': ' + str(arbol[i][1].shape))\n",
    "    # print 'Profundidad ' + str(i + 1) + ': ' + str(arbol[i][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from senfis.autoFIS.autoFIS.autoFIS_one_zip import cv_onezip\n",
    "import timeit\n",
    "\n",
    "\n",
    "# # # # # # # \n",
    "# Run autoFIS old method\n",
    "# # # # # # # \n",
    "\n",
    "def define_parameters(pars):\n",
    "    try:\n",
    "        parameters = [categorical_mask] + pars  # addition of 2 list\n",
    "    except KeyError:\n",
    "        print (\"The database \" + database_name + \" was not found.\\nIt was assumed that all attributes are numeric\")\n",
    "        parameters = [0]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def run_autofis(current_folder_path):\n",
    "    # -------------------------\n",
    "    # Fuzzification parameters\n",
    "    # -------------------------\n",
    "    # categorical_bool_attributes = [0, 0, 0, 0, 1, 0, 0, 0, 0]  # <<=====\n",
    "    triangular_fuzzification_type = \"tukey\"  # \"tukey\", \"normal\"\n",
    "    num_partitions_by_attribute = 3  # 3, 5, 7\n",
    "    is_enable_negation = 0  # 0, 1\n",
    "\n",
    "    # -------------------------\n",
    "    # Formulation parameters\n",
    "    # -------------------------\n",
    "    t_norm = \"prod\"  # \"min\", \"prod\"\n",
    "    ordem_max_premises = 2\n",
    "    # Area filter parameters:\n",
    "    criteria_area = \"cardinalidade_relativa\"  # \"cardinalidade_relativa\", \"frequencia_relativa\"\n",
    "    area_threshold = 0.1\n",
    "    # PCD filter parameter:\n",
    "    is_enable_pcd = [1, 0]\n",
    "    # Overlapping filter parameters:\n",
    "    is_enable_overlapping = [1, 1]\n",
    "    overlapping_threshold = 0.95\n",
    "\n",
    "    # -------------------------\n",
    "    # Association: ex Splitting\n",
    "    # -------------------------\n",
    "    method_association = \"CD\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Aggregation\n",
    "    # -------------------------\n",
    "    method_aggregation = \"MQR\"  # \"MQR\", \"PMQR\", \"intMQR\", \"CD\", \"PCD\", \"max\"\n",
    "\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    # %% Grouping parameters: %%\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    parameters = [triangular_fuzzification_type, num_partitions_by_attribute, t_norm, is_enable_negation,\n",
    "                  ordem_max_premises, criteria_area, area_threshold, is_enable_pcd,\n",
    "                  is_enable_overlapping, overlapping_threshold,\n",
    "                  method_association, method_aggregation]\n",
    "\n",
    "    # =============================================================================================== #\n",
    "    # current_folder_path = os.path.dirname(os.path.realpath(__file__))\n",
    "    current_folder_path = current_folder_path\n",
    "    print (current_folder_path)\n",
    "\n",
    "    databases = []\n",
    "\n",
    "    for archivo in os.listdir(current_folder_path):\n",
    "        if archivo.endswith(\"_csv.zip\"):\n",
    "            databases.append(archivo)\n",
    "    print (databases)\n",
    "    # =============================================================================================== #\n",
    "\n",
    "    file_times = open(os.path.join(current_folder_path, \"Experimento_times.csv\"), 'w')\n",
    "    file_times.write(\"Dataset\" + \", \" + \"Time(s)\" + '\\n')\n",
    "    file_times.close()\n",
    "    # Evaluate each database (zip file)\n",
    "    for data in databases:\n",
    "        t0 = timeit.default_timer()\n",
    "        try:\n",
    "            # data_name_key = data[0:-16]\n",
    "            parameters_database = define_parameters(parameters)\n",
    "            achievement = cv_onezip(current_folder_path, data, parameters_database)\n",
    "            if achievement == 0:\n",
    "                raise ValueError(\"Problems in database: \" + \"<\" + data + \">\")\n",
    "        except ValueError as e:\n",
    "            print (e)\n",
    "            achievement = 0\n",
    "        tf = timeit.default_timer()\n",
    "\n",
    "        if achievement:\n",
    "            file_times = open(os.path.join(current_folder_path, \"Experimento_times.csv\"), 'a')\n",
    "            file_times.write(data[:-16] + ', ' + str(tf - t0) + '\\n')\n",
    "            file_times.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este arquivo executa o processamento de uma base de dados, utilizando validação cruzada.\n",
    "# A base de dados (um arquivo zip) já é separada em 10 splits (em csv) para a validação cruzada.\n",
    "\n",
    "__author__ = 'jparedes'\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from .autoFIS_one_cv import autofis_onecv\n",
    "from numpy import mean, std\n",
    "\n",
    "\n",
    "def cv_onezip(path_databases, zip_file_name, parameters, path_output=0):\n",
    "    # Este arquivo executa o processamento de uma base de dados, utilizando validação cruzada.\n",
    "    # A base de dados (um arquivo zip) já é separada em 10 splits (em csv) para a validação cruzada.\n",
    "    if path_output == 0:\n",
    "        path_output = path_databases\n",
    "    zipFilePath = os.path.join(path_databases, zip_file_name)\n",
    "\n",
    "    # ==================================================================== #\n",
    "    try:\n",
    "        with zipfile.ZipFile(zipFilePath, 'r') as z:\n",
    "            files_cv = z.namelist()\n",
    "\n",
    "        number_files_zip = len(files_cv)\n",
    "        if not (number_files_zip == 20 or number_files_zip == 10):\n",
    "            raise ValueError(\"This module works with a zip file to 10cv or 5cv. \"\n",
    "                             \"For this reason, it is expected 20 or 10 files inside the zip file\")\n",
    "        elif number_files_zip == 20:\n",
    "            a = files_cv[2:] + files_cv[0:2]\n",
    "        else:  # number_files_zip == 10\n",
    "            a = files_cv\n",
    "\n",
    "        list_train, list_test = a[::2], a[1::2]\n",
    "\n",
    "        msg = []\n",
    "        number_cv_pairs = int(number_files_zip / 2)\n",
    "        ac_train = number_cv_pairs * [0]\n",
    "        ac_test = number_cv_pairs * [0]\n",
    "        auc_train = number_cv_pairs * [0]\n",
    "        auc_test = number_cv_pairs * [0]\n",
    "\n",
    "        num_rules = number_cv_pairs * [0]\n",
    "        total_rule_length = number_cv_pairs * [0]\n",
    "\n",
    "        for i in range(number_cv_pairs):\n",
    "            print('Fold nº: ',i)\n",
    "            train_file = list_train[i]\n",
    "            test_file = list_test[i]\n",
    "\n",
    "            message, indicators = autofis_onecv(zipFilePath, train_file, test_file, parameters)\n",
    "            msg.append(message)\n",
    "\n",
    "            if indicators[0] == 0:\n",
    "                name_error = os.path.join(path_output, 'ERROR') + zip_file_name[:-13]\n",
    "                fail_error = open(name_error, 'w')\n",
    "                fail_error.write('Error in CV:' + str(i + 1))\n",
    "                fail_error.write(\"\\n\" + message)\n",
    "                fail_error.close()\n",
    "                raise ValueError(\"Problem detected in CV \" + str(i + 1))\n",
    "\n",
    "            ac_train[i], ac_test[i] = indicators[1][0], indicators[1][1]\n",
    "            auc_train[i], auc_test[i] = indicators[1][2], indicators[1][3]\n",
    "            num_rules[i] = indicators[1][4][0]\n",
    "            total_rule_length[i] = indicators[1][4][1]\n",
    "\n",
    "        filename = os.path.join(path_output, 'Report of ') + zip_file_name[:-8]\n",
    "        target = open(filename, 'w')\n",
    "        target.write('Parameters: ' + str(parameters))\n",
    "        for i2 in range(number_cv_pairs):\n",
    "            target.write('\\n\\n' + str(4 * '===============================') + '\\n\\n')\n",
    "            target.write('CV-' + str(i2 + 1) + '\\n')\n",
    "            target.write('\\n'.join(msg[i2]))\n",
    "\n",
    "        target.write('\\n\\n' + str(4 * '===============================') + '\\n\\n')\n",
    "        target.write('Accuracy training: ' + str(mean(ac_train)) + ', ' + str(std(ac_train)) + '\\n')\n",
    "        target.write('Accuracy testing: ' + str(mean(ac_test)) + ', ' + str(std(ac_test)) + '\\n')\n",
    "        target.write('AUC training: ' + str(mean(auc_train)) + ', ' + str(std(auc_train)) + '\\n')\n",
    "        target.write('AUC testing: ' + str(mean(auc_test)) + ', ' + str(std(auc_test)) + '\\n')\n",
    "        target.write('Number of rules: ' + str(mean(num_rules)) + '\\n')\n",
    "        target.write('Total Rule Length: ' + str(mean(total_rule_length)))\n",
    "        target.close()\n",
    "\n",
    "        achievement = 1\n",
    "\n",
    "        print (\"win \", zip_file_name)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print (e)\n",
    "        achievement = 0\n",
    "\n",
    "    return achievement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este arquivo executa o autoFIS para um fold da validação cruzada\n",
    "\n",
    "import autoFIS.autoFIS.utils_autofis as toolfis\n",
    "# import .utils_autofis as toolfis\n",
    "from autoFIS.autoFIS.formul.autoFIS.formulation import Formulation\n",
    "from autoFIS.autoFIS.association import Association\n",
    "from autoFIS.autoFIS.aggregation import Aggregation\n",
    "from autoFIS.autoFIS.decisions import Decisions\n",
    "from autoFIS.autoFIS.evaluation import Evaluation\n",
    "\n",
    "\n",
    "def autofis_onecv(file_zip, file_train, file_test, parameters):\n",
    "\n",
    "    # General parameters\n",
    "    t_norm = parameters[3]\n",
    "    max_size_of_premise = parameters[5]\n",
    "    association_method = parameters[11]\n",
    "    aggregation_method = parameters[12]\n",
    "\n",
    "    # Gathering parameters\n",
    "    # Formulation parameters:\n",
    "    par_area, par_over, par_pcd = toolfis.get_formulation_parameters(parameters)\n",
    "\n",
    "    # 1. Lecture & Fuzzification\n",
    "    out1 = toolfis.lecture_fuz_one_cv(file_zip, file_train, file_test, parameters)\n",
    "    ux_train, cbin_train = out1[0]\n",
    "    ux_test, cbin_test = out1[1]\n",
    "    num_premises_by_attribute, premises_by_attribute, ref_attributes, premises_contain_negation = out1[2]\n",
    "    freq_classes = out1[3]\n",
    "\n",
    "    report = []  # To save our results\n",
    "\n",
    "    try:\n",
    "        # 3. Formulation\n",
    "        f2 = Formulation(ux_train, cbin_train, ref_attributes, premises_by_attribute,\n",
    "                         num_premises_by_attribute, premises_contain_negation)\n",
    "        # Inputs given by user\n",
    "        arbol = f2.gen_ARB(max_size_of_premise, t_norm, par_area, par_over, par_pcd)\n",
    "\n",
    "        status = [0 if not i[0] else 1 for i in arbol]\n",
    "        sum_status = sum(status)\n",
    "        if sum_status != len(arbol):\n",
    "            if sum_status == 0:\n",
    "                raise ValueError(\"Error in Formulation Module. Any premise survived. \"\n",
    "                                 \"Sorry, you can not continue in the next stage.\"\n",
    "                                 \"\\nTry to change the configuration\")\n",
    "            else:\n",
    "                arb = [i for i in arbol if i[0]]\n",
    "                arbol, arb = arb, arbol\n",
    "        print('Done with Formulation...')\n",
    "        number_classes = cbin_train.shape[1]\n",
    "\n",
    "        report.append(\"\\nFormulation:\\n-----------------\")\n",
    "        report.append(\"Elementos acorde a la profundidad \" + str(len(arbol)) + \" del arbol\")\n",
    "        for i in range(len(arbol)):\n",
    "            report.append('Profundidad ' + str(i + 1) + ': ' + str(arbol[i][1].shape))\n",
    "            # print 'Profundidad ' + str(i + 1) + ': ' + str(arbol[i][1].shape)\n",
    "\n",
    "        # 4. Association: ex-Division\n",
    "        f3 = Association(arbol, cbin_train)\n",
    "        premises_ux_by_class = f3.division(association_method)\n",
    "\n",
    "        status = [0 if not i[0] else 1 for i in premises_ux_by_class]\n",
    "        if sum(status) != number_classes:\n",
    "            raise ValueError(\"Error in Division Module. Some classes did not get premises. \"\n",
    "                             \"Sorry, you can not continue in the next stage.\"\n",
    "                             \"\\nTry to change the configuration\")\n",
    "        print('Done with Association...')\n",
    "\n",
    "        # 5. Aggregation:\n",
    "        f4 = Aggregation(premises_ux_by_class, cbin_train)\n",
    "        output_aggregation = f4.aggregation(aggregation_method)\n",
    "\n",
    "        premises_weights_names = output_aggregation[0]\n",
    "        estimation_classes = output_aggregation[1]\n",
    "\n",
    "        status = [0 if not i[0] else 1 for i in premises_weights_names]\n",
    "        if sum(status) != number_classes:\n",
    "            raise ValueError(\"Error in Aggregation Module. Some classes did not get premises. \"\n",
    "                             \"Sorry, you can not continue in the next stage.\"\n",
    "                             \"\\nTry to change the configuration\")\n",
    "        print('Done with Aggregation...')\n",
    "\n",
    "        final_premises_classes = []\n",
    "        report.append(\"\\n\\nPremises:\\n=========\")\n",
    "        for i in range(len(premises_weights_names)):\n",
    "            report.append(\"Premises of Class \" + str(i) + \": \" + str(premises_weights_names[i][0]))\n",
    "            final_premises_classes.append(premises_weights_names[i][0])\n",
    "            report.append(\"weights_\" + str(i) + \": \" + str(premises_weights_names[i][1].T))\n",
    "\n",
    "        # 6. Decision:\n",
    "        f5 = Decisions(estimation_classes, freq_classes)\n",
    "        train_bin_prediction = f5.dec_max_pert()\n",
    "        print('Done with Decision...')\n",
    "\n",
    "        # 7. Evaluation\n",
    "        f6 = Evaluation(premises_weights_names, final_premises_classes, freq_classes)\n",
    "        metrics_train = f6.eval_train(cbin_train, train_bin_prediction)\n",
    "        metrics_test = f6.eval_test(cbin_test, ux_test, t_norm)\n",
    "        print('Done with Evaluation...')\n",
    "\n",
    "        report.append(\"\\nEvaluation Training:\\n---------------------------\")\n",
    "        report.append(\"Accuracy on train dataset: \" + str(metrics_train[0]))\n",
    "        report.append(\"AUC in train dataset: \" + str(metrics_train[1]))\n",
    "        report.append(\"Recall: \" + str(metrics_train[3]))\n",
    "        report.append('Confusion matrix:\\n' + str(metrics_train[2]))\n",
    "\n",
    "        report.append(\"\\nEvaluation Testing:\\n---------------------------\")\n",
    "        report.append(\"Accuracy on test dataset: \" + str(metrics_test[0]))\n",
    "        report.append(\"AUC in test dataset: \" + str(metrics_test[1]))\n",
    "        report.append(\"Recall: \" + str(metrics_test[3]))\n",
    "        report.append(\"Confusion matrix:\\n\" + str(metrics_test[2]))\n",
    "\n",
    "        # Metrics to eval: accuracy_test, auc_test,\n",
    "        #                  [num_regras, total_rule_length, tamano_medio_das_regras]]\n",
    "        metricas = [1, [metrics_train[0], metrics_test[0], metrics_train[1], metrics_test[1], metrics_test[4]]]\n",
    "\n",
    "    except ValueError as e:\n",
    "        print (e)\n",
    "        report = e  # .append(\"\\n\" + str(e))\n",
    "        metricas = [0, \"No se termino el proceso, se detuvo en algun etapa\"]\n",
    "\n",
    "    return report, metricas\n",
    "\n",
    "\n",
    "def main():\n",
    "    filezip_name = \"D:\\\\Jorg\\Projects\\\\autoFIS\\\\test\\\\datas\" + '\\\\' + 'saheart-10-fold_csv.zip'\n",
    "    train_file = \"saheart-10-7tra.csv\"\n",
    "    test_file = \"saheart-10-7tst.csv\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Fuzzification parameters\n",
    "    # -------------------------\n",
    "    categorical_bool_attributes = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    triangular_fuzzification_type = \"normal\"  # \"tukey\", \"normal\"\n",
    "    num_partitions_by_attribute = 3\n",
    "    t_norm = \"min\"  # \"min\", \"prod\"\n",
    "    is_enable_negation = 0  # 0, 1\n",
    "\n",
    "    # -------------------------\n",
    "    # Formulation parameters\n",
    "    # -------------------------\n",
    "    ordem_max_premises = 2\n",
    "    # Area filter parameters:\n",
    "    criteria_area = \"cardinalidade_relativa\"  # \"cardinalidade_relativa\", \"frequencia_relativa\"\n",
    "    area_threshold = 0.05\n",
    "    # PCD filter parameter:\n",
    "    is_enable_pcd = 1\n",
    "    # Overlapping filter parameters:\n",
    "    is_enable_overlapping = 1\n",
    "    overlapping_threshold = 0.95\n",
    "\n",
    "    # -------------------------\n",
    "    # Association\n",
    "    # -------------------------\n",
    "    method_association = \"MQR\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq_max\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Aggregation\n",
    "    # -------------------------\n",
    "    method_aggregation = \"MQR\"  # \"MQR\", \"PMQR\", \"CD\", \"PCD\", \"freq_max\"\n",
    "\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    # %% Grouping parameters: %%\n",
    "    # %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    parameters = [categorical_bool_attributes, triangular_fuzzification_type,\n",
    "                  num_partitions_by_attribute, t_norm, is_enable_negation,\n",
    "                  ordem_max_premises, criteria_area, area_threshold, is_enable_pcd,\n",
    "                  is_enable_overlapping, overlapping_threshold,\n",
    "                  method_association, method_aggregation]\n",
    "\n",
    "    result_1cv = autofis_onecv(filezip_name, train_file, test_file, parameters)\n",
    "    print (result_1cv[1])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodos auxiliares\n",
    "\n",
    "def get_formulation_parameters(parameters):\n",
    "    criteria_area = parameters[6]\n",
    "    area_threshold = parameters[7]\n",
    "    is_enable_pcd = parameters[8]\n",
    "    is_enable_overlapping = parameters[9]\n",
    "    overlapping_threshold = parameters[10]\n",
    "    par_area = [criteria_area, area_threshold]\n",
    "    par_over = [is_enable_overlapping, overlapping_threshold]\n",
    "    par_pcd = is_enable_pcd\n",
    "    return par_area, par_over, par_pcd\n",
    "\n",
    "\n",
    "def lecture_fuz_one_cv(zipFilePath, file_train, file_test, parameters):\n",
    "    # Parameters\n",
    "    cat_bool, fz_type, fz_number_partition = parameters[0:3]\n",
    "    is_enable_negation = parameters[4]\n",
    "    # Lecture\n",
    "    reader = Lecture()\n",
    "    reader.read_1cv(zipFilePath, file_train, file_test)\n",
    "    # [x, y_Bin, Freq_Class, Dic_Labels, Dic_Class, Index_Train]\n",
    "    x, y_bin, freq_classes, _, _, index_train = reader.info_data()\n",
    "\n",
    "    # Fuzzification\n",
    "    matrix_x = x.copy()\n",
    "    fuzzifier = Fuzzification(matrix_x, cat_bool)\n",
    "    fuzzifier.build_uX(fz_type, fz_number_partition)\n",
    "    if is_enable_negation == 1:\n",
    "        fuzzifier.add_negation()\n",
    "    print('Successful FZ')\n",
    "    # Getting train and test partitions\n",
    "    ux_train = fuzzifier.uX[:index_train, :]\n",
    "    ux_test = fuzzifier.uX[index_train:, :]\n",
    "    cbin_train = y_bin[:index_train, :]\n",
    "    cbin_test = y_bin[index_train:, :]\n",
    "\n",
    "    # Information about attributes fuzzification\n",
    "    sizes_attributes = fuzzifier.num_of_premises_by_attribute  # [3, 2, 3]\n",
    "    # [(0,1,2),(3,4),(5,6,7)]\n",
    "    premises_by_attribute = fuzzifier.attribute_premises\n",
    "    ref_attributes = fuzzifier.ref_attributes  # [0, 1, 2]\n",
    "    premises_contain_negation = fuzzifier.indexes_premises_contain_negation\n",
    "\n",
    "    fuz_train = [ux_train, cbin_train]\n",
    "    fuz_test = [ux_test, cbin_test]\n",
    "    attributes_information = [\n",
    "        sizes_attributes, premises_by_attribute, ref_attributes, premises_contain_negation]\n",
    "# return fuz_train, fuz_test, attributes_information, freq_classes, gain_by_att\n",
    "    return fuz_train, fuz_test, attributes_information, freq_classes"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('puc': venv)",
   "language": "python",
   "name": "python37764bitpucvenv3c8e04841bf343089962c3369eba30a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}